{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db29060a",
   "metadata": {},
   "source": [
    "# 1.ViT图像语言推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6bef362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b863439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38f117a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to input directory containing images\n",
    "input_dir = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/baidu2023_pinjie\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Move model to CUDA device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "def predict(input_dir):\n",
    "  # images = []\n",
    "  for image_path in tqdm(Path(input_dir).glob(\"*.png\")):\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    # Pre-process the image for the DETR model\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "\n",
    "  return preds\n",
    "  # all_data.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3e50ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15775it [1:17:10,  3.41it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "1 columns passed, passed data had 41 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:969\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:1017\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m   1018\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[39melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 1 columns passed, passed data had 41 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mE:/Dataset/GNN_Perception/wuhan_badu_SVI/2023/predictions.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Save data to CSV file\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame\u001b[39m.\u001b[39;49mfrom_records(predict(input_dir), columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mfilename\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      8\u001b[0m df\u001b[39m.\u001b[39mset_index(\u001b[39m\"\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m\"\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m df\u001b[39m.\u001b[39mto_csv(output_file)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\frame.py:2338\u001b[0m, in \u001b[0;36mDataFrame.from_records\u001b[1;34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[0m\n\u001b[0;32m   2336\u001b[0m     arr_columns \u001b[39m=\u001b[39m columns\n\u001b[0;32m   2337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2338\u001b[0m     arrays, arr_columns \u001b[39m=\u001b[39m to_arrays(data, columns)\n\u001b[0;32m   2339\u001b[0m     \u001b[39mif\u001b[39;00m coerce_float:\n\u001b[0;32m   2340\u001b[0m         \u001b[39mfor\u001b[39;00m i, arr \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(arrays):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    872\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    873\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 875\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:972\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m    975\u001b[0m     contents \u001b[39m=\u001b[39m _convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 1 columns passed, passed data had 41 columns"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define path to output CSV file\n",
    "output_file = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/2023/predictions.csv\"\n",
    "\n",
    "# Save data to CSV file\n",
    "df = pd.DataFrame.from_records(predict(input_dir), columns=[\"filename\"])\n",
    "df.set_index(\"filename\", inplace=True)\n",
    "df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c37cff0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# 将结果写入CSV文件\n",
    "# 将预测结果写入CSV文件\n",
    "with open('E:/Dataset/GNN_Perception/wuhan_badu_SVI/2023/predictions.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # 写入表头\n",
    "    writer.writerow(['Predictions'])\n",
    "    \n",
    "    preds = predict(input_dir)\n",
    "    # 逐行写入数据\n",
    "    grouped_preds = [preds[i:i+4] for i in range(0, len(preds), 4)]\n",
    "    for group_preds in grouped_preds:\n",
    "        predictions_str = ', '.join([json.dumps(pred) for pred in group_preds])\n",
    "        writer.writerow([predictions_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dbc6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(image_paths):\n",
    "  # images = []\n",
    "  for image_path in image_paths:\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    # Pre-process the image for the DETR model\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    # Use the DETR model to predict object bounding boxes and labels\n",
    "    # outputs = model(**inputs)\n",
    "\n",
    "    # i_image = Image.open(image_path)\n",
    "    # if i_image.mode != \"RGB\":\n",
    "    #   i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    # images.append(i_image)\n",
    "\n",
    "    # 更新读取图片进度\n",
    "    total_bar.update(1);\n",
    "    #total_bar.write(f\"{total_bar.n}/{total_bar.total} images processed\") \n",
    "    \n",
    "  # pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  # pixel_values = pixel_values.to(device)\n",
    "\n",
    "  output_ids = model.generate(inputs, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75074bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images: 100%|█████████▉| 15772/15775 [11:29<00:00, 25.82it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\feature_extraction_utils.py:90\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[0;32m     91\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# 设置总体进度条\u001b[39;00m\n\u001b[0;32m      4\u001b[0m total_bar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(image_list), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPredicting images\u001b[39m\u001b[39m'\u001b[39m, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m preds \u001b[39m=\u001b[39m predict_step(image_list)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(preds)\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mpredict_step\u001b[1;34m(image_paths)\u001b[0m\n\u001b[0;32m     19\u001b[0m   total_bar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m);\n\u001b[0;32m     20\u001b[0m   \u001b[39m#total_bar.write(f\"{total_bar.n}/{total_bar.total} images processed\") \u001b[39;00m\n\u001b[0;32m     21\u001b[0m   \n\u001b[0;32m     22\u001b[0m \u001b[39m# pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# pixel_values = pixel_values.to(device)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m output_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgen_kwargs)\n\u001b[0;32m     27\u001b[0m preds \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(output_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m preds \u001b[39m=\u001b[39m [pred\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m preds]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1239\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[39m# 3. Define model inputs\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[0;32m   1234\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m   1236\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_model_inputs(\n\u001b[0;32m   1237\u001b[0m     inputs, generation_config\u001b[39m.\u001b[39mbos_token_id, model_kwargs\n\u001b[0;32m   1238\u001b[0m )\n\u001b[1;32m-> 1239\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1241\u001b[0m \u001b[39m# 4. Define other model kwargs\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_attentions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39moutput_attentions\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\feature_extraction_utils.py:92\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[0;32m     91\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images: 100%|██████████| 15775/15775 [11:40<00:00, 25.82it/s]"
     ]
    }
   ],
   "source": [
    "# 执行预测\n",
    "image_list = [str(p) for p in glob(\"E:/Dataset/GNN_Perception/wuhan_badu_SVI/baidu2023_pinjie/*.png\")]\n",
    "# 设置总体进度条\n",
    "total_bar = tqdm(total=len(image_list), desc='Predicting images', position=0)\n",
    "preds = predict_step(image_list)\n",
    "print(preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ede2fcab",
   "metadata": {},
   "source": [
    "# 2.语义分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a0385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\segformer\\feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n",
      "15775it [27:47,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23min 10s\n",
      "Wall time: 28min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the feature extractor and segmentation model\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\").cuda()\n",
    "\n",
    "# Define path to input directory containing images\n",
    "input_dir = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/baidu2023_pinjie\"\n",
    "\n",
    "# Define path to output CSV file\n",
    "output_file = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/2023/seg.csv\"\n",
    "\n",
    "# Get list of class names as column names\n",
    "class_names = list(model.config.id2label.values())[1:]\n",
    "\n",
    "# Loop over all image files in the input directory\n",
    "all_data = []\n",
    "for image_path in tqdm(Path(input_dir).glob(\"*.png\")):\n",
    "    # Load image using PIL\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Pre-process the image for the Segformer model\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Use the Segformer model to predict pixel-wise class labels\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "    predictions = torch.argmax(logits, dim=1)  # shape (batch_size, height/4, width/4)\n",
    "\n",
    "    # Compute object proportions\n",
    "    class_proportions = {cls: 0.0 for cls in class_names}\n",
    "    total_pixels = predictions.shape[1] * predictions.shape[2]\n",
    "    for i, cls_name in enumerate(class_names, start=1):\n",
    "        mask = (predictions[0] == i).to(torch.float)\n",
    "        count = mask.sum().item()\n",
    "        proportion = round(count / total_pixels * 100, 2)\n",
    "        class_proportions[cls_name] = proportion\n",
    "        #print(f\"Class {cls_name} has {class_proportions[cls_name]} of image pixels\")\n",
    "\n",
    "    # Append data to list of all data\n",
    "    data = {\"filename\": image_path.name}\n",
    "    data.update(class_proportions)\n",
    "    data[\"total_pixels\"] = total_pixels\n",
    "    all_data.append(data)\n",
    "\n",
    "# Save data to CSV file\n",
    "df = pd.DataFrame.from_records(all_data, columns=[\"filename\"] + class_names + [\"total_pixels\"])\n",
    "df.set_index(\"filename\", inplace=True)\n",
    "df.to_csv(output_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cadbfacb",
   "metadata": {},
   "source": [
    "# 3.物体检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ad12bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:780: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n",
      "15775it [28:51,  9.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 26min 11s\n",
      "Wall time: 28min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define path to input directory containing images\n",
    "input_dir = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/baidu2023_pinjie\"\n",
    "\n",
    "# Define path to output CSV file\n",
    "output_file = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/2023/obj.csv\"\n",
    "\n",
    "# Load model\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Move model to CUDA device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loop over all image files in the input directory\n",
    "all_data = []\n",
    "for image_path in tqdm(Path(input_dir).glob(\"*.png\")):\n",
    "    # Load image using PIL\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Pre-process the image for the DETR model\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "\n",
    "    # Use the DETR model to predict object bounding boxes and labels\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Convert outputs (bounding boxes and class logits) to COCO API format\n",
    "    # Let's only keep detections with score > 0.9\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "    # Compute object counts\n",
    "    counts = {cls: 0 for cls in model.config.id2label.values()}\n",
    "    for label in results[\"labels\"]:\n",
    "        cls_name = model.config.id2label[label.item()]\n",
    "        counts[cls_name] += 1\n",
    "\n",
    "    # Append data to list of all data\n",
    "    data = {\"filename\": image_path.name}\n",
    "    data.update(counts)\n",
    "    all_data.append(data)\n",
    "\n",
    "    # Print object predictions\n",
    "    #print(f\"\\nObject Predictions for {image_path.name}:\")\n",
    "    #for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        #cls_name = model.config.id2label[label.item()]\n",
    "        #if score.cpu().item() > 0.9:\n",
    "            #box = [round(i, 2) for i in box.tolist()]\n",
    "            #print(f\"Detected {cls_name} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "# Save data to CSV file\n",
    "df = pd.DataFrame.from_records(all_data, columns=[\"filename\"] + list(model.config.id2label.values()))\n",
    "df.set_index(\"filename\", inplace=True)\n",
    "df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dd4137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "print(scores.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82a3b3f2",
   "metadata": {},
   "source": [
    "# 4.深度估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577eace1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (512,2048) (512,1536) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m depths2 \u001b[39m=\u001b[39m prediction2\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     68\u001b[0m \u001b[39m# calculate depth distance in meters\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m distance \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(depths1 \u001b[39m-\u001b[39;49m depths2)\u001b[39m.\u001b[39mmean() \u001b[39m/\u001b[39m \u001b[39m1000.0\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39m# write depth distance to CSV file\u001b[39;00m\n\u001b[0;32m     72\u001b[0m row \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(path1), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(path2), distance]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (512,2048) (512,1536) "
     ]
    }
   ],
   "source": [
    "#不太适用\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "# directory containing the images\n",
    "image_dir = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/baidu2023_pinjie/\"\n",
    "\n",
    "# create a new CSV file to store depth distances\n",
    "with open(\"E:/Dataset/GNN_Perception/wuhan_badu_SVI/2023/depth_distances.csv\", mode=\"w\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # list all image files in the directory\n",
    "    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # loop through the images and calculate depth distances\n",
    "    for i, path1 in enumerate(image_paths):\n",
    "        # open the first image\n",
    "        image1 = Image.open(path1)\n",
    "\n",
    "        # prepare image for the model\n",
    "        inputs1 = processor(images=image1, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model(**inputs1)\n",
    "            predicted_depth1 = outputs1.predicted_depth\n",
    "\n",
    "        # interpolate to original size\n",
    "        prediction1 = torch.nn.functional.interpolate(\n",
    "            predicted_depth1.unsqueeze(1),\n",
    "            size=image1.size[::-1],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        # get depth values as a numpy array\n",
    "        depths1 = prediction1.squeeze().cpu().numpy()\n",
    "\n",
    "        # loop through the rest of the images and calculate distance to each one\n",
    "        for j, path2 in enumerate(image_paths[i+1:], i+1):\n",
    "            # open the second image\n",
    "            image2 = Image.open(path2)\n",
    "\n",
    "            # prepare image for the model\n",
    "            inputs2 = processor(images=image2, return_tensors=\"pt\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs2 = model(**inputs2)\n",
    "                predicted_depth2 = outputs2.predicted_depth\n",
    "\n",
    "            # interpolate to original size\n",
    "            prediction2 = torch.nn.functional.interpolate(\n",
    "                predicted_depth2.unsqueeze(1),\n",
    "                size=image2.size[::-1],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "\n",
    "            # get depth values as a numpy array\n",
    "            depths2 = prediction2.squeeze().cpu().numpy()\n",
    "\n",
    "            # calculate depth distance in meters\n",
    "            distance = np.abs(depths1 - depths2).mean() / 1000.0\n",
    "                        \n",
    "            # write depth distance to CSV file\n",
    "            row = [os.path.basename(path1), os.path.basename(path2), distance]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e8082d3",
   "metadata": {},
   "source": [
    "# 5.Opencv提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18b8776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15775/15775 [15:33<00:00, 16.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义要处理的文件夹路径\n",
    "folder_path = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/baidu2023_pinjie\"\n",
    "\n",
    "# 定义输出CSV文件路径\n",
    "output_file_path = \"E:/Dataset/GNN_Perception/wuhan_badu_SVI/2023/pixels.csv\"\n",
    "\n",
    "# 遍历文件夹中的所有文件\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # 读取图像\n",
    "        img = cv2.imread(os.path.join(folder_path, filename))\n",
    "        \n",
    "        # 转换为HSV颜色空间\n",
    "        hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # 提取色相、饱和度、亮度\n",
    "        h, s, v = cv2.split(hsv_img)\n",
    "\n",
    "        # 提取边缘\n",
    "        edges = cv2.Canny(img, 100, 200)\n",
    "        \n",
    "        # 获取阈值\n",
    "        ret, thresh = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # 将处理后的数据存储到CSV文件中\n",
    "        with open(output_file_path, mode='a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            row = [filename, h.mean(), s.mean(), v.mean(), edges.mean(), thresh.mean()]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f72822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# 定义要处理的文件夹路径\n",
    "folder_path = \"E:/Dataset/GNN_Perception/SVI/remove\"\n",
    "\n",
    "# 定义输出CSV文件路径\n",
    "output_file_path = \"E:/Dataset/GNN_Perception/SVI/wuhan_pixels.csv\"\n",
    "\n",
    "# 遍历文件夹中的所有文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # 读取图像\n",
    "        img = cv2.imread(os.path.join(folder_path, filename))\n",
    "        \n",
    "        # 检查输入图像是否为空\n",
    "        if img is None:\n",
    "            print(f\"Failed to read image {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # 转换为HSV颜色空间\n",
    "        hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # 提取色相、饱和度、亮度\n",
    "        h, s, v = cv2.split(hsv_img)\n",
    "\n",
    "        # 提取边缘\n",
    "        edges = cv2.Canny(img, 100, 200)\n",
    "        \n",
    "        # 获取阈值\n",
    "        ret, thresh = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # 将处理后的数据存储到CSV文件中\n",
    "        with open(output_file_path, mode='a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            row = [filename, h.mean(), s.mean(), v.mean(), edges.mean(), thresh.mean()]\n",
    "            writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
